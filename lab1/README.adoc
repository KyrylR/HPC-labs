= Matrix-Vector Multiplication Performance Analysis

== Introduction

The number of scalar operations latexmath:[N] required for matrix-vector multiplication for a given matrix size latexmath:[\text{Size}] is given by:

latexmath:[N = \text{Size} \times \text{Size} \times 2 - \text{Size}]

This formula captures both the multiplications and the additions required for the multiplication of a matrix row with a vector.


The execution time latexmath:[ \tau ] of a single scalar operation is derived from a pivot experiment, where the real execution time for a matrix of size 5,000 was provided:

latexmath:[ \tau = \frac{\text{Real Execution Time for Size 5,000}}{N} ]

Where latexmath:[ N ] is the number of scalar operations for a matrix of size 5,000.

=== Theoretical Execution Time

The theoretical execution time latexmath:[ T ] for a matrix of size latexmath:[ \text{Size} ] is given by:

latexmath:[ T = \tau \times (\text{Size} \times \text{Size} \times 2 - \text{Size}) ]

== Results

=== Theoretical Execution Time Calculation

Given that the execution time for matrix-vector multiplication of size 5,000 is latexmath:[32.898] ms (or latexmath:[0.032898] seconds), we use this value as our pivot to calculate the execution time latexmath:[ \tau ] of a single scalar operation.

Using the provided formula:
latexmath:[ N = \text{Size} \times \text{Size} \times 2 - \text{Size} ]

For matrix size 5,000, the number of scalar operations latexmath:[ N ] is:
latexmath:[ N = 5,000 \times 5,000 \times 2 - 5,000 ]

Now, latexmath:[ \tau = \frac{\text{Real Execution Time for Size 5,000}}{N} ]
latexmath:[ \tau = \frac{0.018013}{N} ]

Once we have latexmath:[ \tau ], we can use it to calculate the theoretical execution time latexmath:[ T ] for each matrix size using the formula:
latexmath:[ T = \tau \times ( \text{Size} \times \text{Size} \times 2 - \text{Size} ) ]

Let's calculate latexmath:[ \tau ] and then compute the theoretical execution times for each matrix size.

We have the execution time latexmath:[ \tau ] of a single scalar operation as approximately latexmath:[ 3.60 \times 10^{-10} ] seconds.

Using this value, we can calculate the theoretical execution times for each matrix size as follows:

The conversions are as follows:

* 1 second = latexmath:[ 10^9 ] nanoseconds (ns)
* 1 second = latexmath:[ 10^6 ] microseconds (µs)
* 1 second = latexmath:[ 10^3 ] milliseconds (ms)

.Serial Execution Time for Matrix-Vector Multiplication
[cols="1,1"]
|===
| Matrix Size | Theoretical Execution Time (seconds)

| 10          | 68.456 ns
| 100         | 7.1699 µs
| 1,000       | 720.23 µs
| 2,000       | 2.8816 ms
| 3,000       | 6.4842 ms
| 4,000       | 11.528 ms
| 5,000       | 18.013 ms
| 6,000       | 25.939 ms
| 7,000       | 35.306 ms
| 8,000       | 46.115 ms
| 9,000       | 58.365 ms
| 10,000      | 72.056 ms
|===

=== Comparison of Real and Theoretical Execution Times

Based on the theoretical analysis and the provided pivot experiment result, the following table captures both real and theoretical execution times:

.Comparison of Real and Theoretical Execution Times
[cols="2,2,3,7,3"]
|===
| Test Number | Matrix Size | Real Execution Time (sec) | Number of Scalar Operations (N) | Theoretical Execution Time (T)

| 1           | 10          | 98.458 ns                  | 10 × 10 × 2 - 10                 | 68.456 ns
| 2           | 100         | 5.3605 µs                  | 100 × 100 × 2 - 100              | 7.1699 µs
| 3           | 1,000       | 734.89 µs                  | 1,000 × 1,000 × 2 - 1,000        | 720.23 µs
| 4           | 2,000       | 2.9529 ms                  | 2,000 × 2,000 × 2 - 2,000        | 2.8816 ms
| 5           | 3,000       | 7.0897 ms                  | 3,000 × 3,000 × 2 - 3,000        | 6.4842 ms
| 6           | 4,000       | 11.735 ms                  | 4,000 × 4,000 × 2 - 4,000        | 11.528 ms
| 7           | 5,000       | 18.013 ms                  | 5,000 × 5,000 × 2 - 5,000        | 18.013 ms
| 8           | 6,000       | 25.666 ms                  | 6,000 × 6,000 × 2 - 6,000        | 25.939 ms
| 9           | 7,000       | 36.644 ms                  | 7,000 × 7,000 × 2 - 7,000        | 35.306 ms
| 10          | 8,000       | 47.703 ms                  | 8,000 × 8,000 × 2 - 8,000        | 46.115 ms
| 11          | 9,000       | 61.346 ms                  | 9,000 × 9,000 × 2 - 9,000        | 58.365 ms
| 12          | 10,000      | 79.330 ms                  | 10,000 × 10,000 × 2 - 10,000     | 72.056 ms
|===


== Parallel Execution Times in comparison to Serial Execution Time

> Data distribution is not counted.

[cols="2,2,2,2,2"]
|===
| Matrix Size | Serial       | 2 processors | 4 processors | 8 processors
| 10          | 98.458 ns    | 9.267 µs     | 63.379 µs    | 45.951 µs
| 100         | 5.3605 µs    | 10.76 µs     | 20.479 µs    | 30.529 µs
| 1,000       | 734.89 µs    | 327.51 µs    | 546.37 µs    | 213.171 µs
| 2,000       | 2.9529 ms    | 1.4954 ms    | 764.92 µs    | 612.789 µs
| 3,000       | 7.0897 ms    | 3.2750 ms    | 1.9989 ms    | 1.31920 ms
| 4,000       | 11.735 ms    | 7.6964 ms    | 3.8310 ms    | 2.67196 ms
| 5,000       | 18.013 ms    | 10.273 ms    | 6.1414 ms    | 3.61441 ms
| 6,000       | 25.666 ms    | 16.129 ms    | 6.7150 ms    | 5.72066 ms
| 7,000       | 36.644 ms    | 21.329 ms    | 11.416 ms    | 7.66839 ms
| 8,000       | 47.703 ms    | 29.898 ms    | 14.586 ms    | 10.6725 ms
| 9,000       | 61.346 ms    | 36.537 ms    | 19.761 ms    | 13.0965 ms
| 10,000      | 79.330 ms    | 43.758 ms    | 25.848 ms    | 17.3598 ms
| 30,000      | 683.78 ms    | 395.63 ms    | 213.79 ms    | 143.395 ms
|===

== Speedup in comparison to Serial Execution Time

[cols="2,2,2,2"]
|===
| Matrix Size | 2 processors | 4 processors | 8 processors
| 10          | 0.0106       | 0.0016       | 0.0021
| 100         | 0.4982       | 0.2618       | 0.1756
| 1,000       | 2.244        | 1.345        | 3.447
| 2,000       | 1.975        | 3.860        | 4.819
| 3,000       | 2.165        | 3.547        | 5.374
| 4,000       | 1.525        | 3.063        | 4.392
| 5,000       | 1.753        | 2.933        | 4.984
| 6,000       | 1.591        | 3.822        | 4.487
| 7,000       | 1.718        | 3.210        | 4.779
| 8,000       | 1.596        | 3.270        | 4.470
| 9,000       | 1.679        | 3.104        | 4.684
| 10,000      | 1.813        | 3.069        | 4.570
| 30,000      | 1.728        | 3.198        | 4.769
|===

== Comparison of Real vs Scaled Theoretical Execution Times (in seconds)

The constants used for this adjustment are:

- latexmath:[\alpha] (latency) = 0.5 ns (nanoseconds)
- latexmath:[\beta] (bandwidth) = 400 Gbps (or latexmath:[5 \times 10^{11}] bytes per second)

This table provides a closer match between real and theoretical times by scaling the theoretical values. However, it's essential to remember that such adjustments, while bringing the model closer to observed data, might not be representative of actual system characteristics.

The formula used for theoretical times is:

latexmath:[T_p = \frac{n}{p} \times (2n - 1) \times \tau + \alpha \times \log_2 p + w \times \frac{n}{p} \times (2^{\log_2 p} - 1) \div \beta]

where:
- latexmath:[n] is the matrix size.
- latexmath:[p] is the number of processors.
- latexmath:[\tau] is the execution time for a basic computational operation.
- latexmath:[\alpha] is the latency.
- latexmath:[\beta] is the bandwidth.
- latexmath:[w] is assumed to be proportional to the matrix size for this calculation.

[cols="2,3,3,3"]
|===
| Matrix Size | 2 processors (Real, Scaled Theoretical) | 4 processors (Real, Scaled Theoretical) | 8 processors (Real, Scaled Theoretical)
| 10          | 9.267 µs, 9.354 µs                      | 63.379 µs, 4.678 µs                     | 45.951 µs, 2.340 µs
| 100         | 10.76 µs, 979.657 µs                    | 20.479 µs, 489.830 µs                   | 30.529 µs, 244.916 µs
| 1,000       | 327.51 µs, 98.409 ms                    | 546.37 µs, 49.204 ms                    | 213.171 µs, 24.602 ms
| 2,000       | 1.4954 ms, 393.734 ms                   | 764.92 µs, 196.867 ms                   | 612.789 µs, 98.433 ms
| 3,000       | 3.2750 ms, 885.974 ms                   | 1.9989 ms, 442.987 ms                   | 1.31920 ms, 221.494 ms
| 4,000       | 7.6964 ms, 1.5751 s                     | 3.8310 ms, 787.566 ms                   | 2.67196 ms, 393.783 ms
| 5,000       | 10.273 ms, 2.4612 s                     | 6.1414 ms, 1.2306 s                     | 3.61441 ms, 615.301 ms
| 6,000       | 16.129 ms, 3.5442 s                     | 6.7150 ms, 1.7721 s                     | 5.72066 ms, 886.048 ms
| 7,000       | 21.329 ms, 4.8241 s                     | 11.416 ms, 2.4120 s                     | 7.66839 ms, 1.2060 s
| 8,000       | 29.898 ms, 6.3009 s                     | 14.586 ms, 3.1505 s                     | 10.6725 ms, 1.5752 s
| 9,000       | 36.537 ms, 7.9747 s                     | 19.761 ms, 3.9873 s                     | 13.0965 ms, 1.9937 s
| 10,000      | 43.758 ms, 9.8453 s                     | 25.848 ms, 4.9227 s                     | 17.0671 ms, 2.4918 s

|===

== Conclusion

In our analysis of matrix-vector multiplication using parallel processing, it's evident that the parallel approach can
significantly outperform the serial approach, especially for larger matrix sizes.
However, the degree of improvement varies based on the matrix size and the number of processors involved:

- For smaller matrices, such as the 10x10 matrix, the overhead of parallelization is more prominent, resulting in minimal to no speedup.
This indicates that for trivial tasks, the overhead of parallel execution can outweigh the benefits.

- As the matrix size increases, the benefits of parallel processing become more pronounced. However, even in these cases,
the speedup doesn't always scale linearly with the number of processors, indicating the presence of other limiting factors.

The comparison of real versus scaled theoretical execution times highlights the discrepancies between idealized models
and real-world performance. Theoretical models, while valuable for setting benchmarks and expectations, often don't
capture all the intricacies and overheads of real-world systems.

Such disparities can arise from a variety of factors:

- **Communication Overheads**: As more processors are involved, the need for communication between them can introduce delays.

- **System-Specific Factors**: The actual hardware and software configurations can impact performance.
Factors like cache sizes, memory bandwidth, and inter-processor communication mechanisms play a role.

- **Model Assumptions**: Theoretical models often make simplifying assumptions that may not hold in real scenarios.

In summary, while parallel processing offers substantial advantages, it's crucial to consider the specific use case and
the associated overheads. Future investigations might delve into optimizing the parallelization strategy, exploring
different matrix partitioning techniques, or considering other parallel algorithms to enhance matrix-vector multiplication further.